{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from rekall import Interval, IntervalSet, IntervalSetMapping, Bounds3D\n",
    "from rekall.predicates import *\n",
    "from vgrid import VGridSpec, VideoMetadata, VideoBlockFormat, FlatFormat, SpatialType_Bbox, SpatialType_Keypoints, Metadata_Keypoints\n",
    "from vgrid_jupyter import VGridWidget\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from const import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in video data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_VIDEO = True\n",
    "test_id = 2\n",
    "# load in video metadata\n",
    "video_collection_intel = [\n",
    "    {'num_frames': 3053, 'height': 720, 'width': 406, 'fps': 29.84, 'filename': 'dally_sy.mp4', 'id': 0},\n",
    "    {'num_frames': 1488, 'height': 720, 'width': 1280, 'fps': 30, 'filename': 'hip_emily.mp4', 'id': 1},\n",
    "    {'num_frames': 2062, 'height': 720, 'width': 1280, 'fps': 30, 'filename': '7thsense_mayee.mp4', 'id': 2}\n",
    "    ]\n",
    "if SINGLE_VIDEO:\n",
    "    video_collection_intel = [video_collection_intel[2]]\n",
    "video_metadata_intel = [\n",
    "    VideoMetadata(v[\"filename\"], v[\"id\"], v[\"fps\"], int(v[\"num_frames\"]), v[\"width\"], v[\"height\"])\n",
    "    for v in video_collection_intel\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load in openpose output data\n",
    "json_dir_sy = \"C:/Users/heidi/Documents/seniorproject/openpose-1.5.1-binaries-win64-only_cpu-python-flir-3d/openpose-1.5.1-binaries-win64-only_cpu-python-flir-3d/openpose/long_output/\"\n",
    "json_dir_emily = \"C:/Users/heidi/Documents/seniorproject/data/hip_output/\"\n",
    "json_dir_mayee = \"C:/Users/heidi/Documents/seniorproject/data/7thsense_output/\"\n",
    "json_dirs = [json_dir_sy, json_dir_emily, json_dir_mayee]\n",
    "# json_dirs = [json_dir_mayee]\n",
    "\n",
    "if SINGLE_VIDEO:\n",
    "    json_dirs = [json_dirs[test_id]]\n",
    "\n",
    "def load_openpose_data(keypoint_files, vm):\n",
    "    \"\"\"\n",
    "    Loads openpose data for single video.\n",
    "    Arguments:\n",
    "        keypoint_files: list of keypoint.json file names for one video\n",
    "            each json file contains dictionary of openpose output info for one frame. \n",
    "        vm: VideoMetadata object for one video\n",
    "    Returns:\n",
    "        frame_list: list of frames where each frame is a dictionary of part idxs to points {0: [x y conf], 1: etc.} \n",
    "    \"\"\"\n",
    "    frame_list = []\n",
    "    for js in keypoint_files:\n",
    "        with open(js) as json_file:\n",
    "            keypoint_data = json.load(json_file)\n",
    "#             if (len(keypoint_data['people']) > 1):\n",
    "#                 print (js)\n",
    "            if (len(keypoint_data['people']) != 0 and len(keypoint_data['people'][0]) != 0):\n",
    "                pose_keypoints_2d = keypoint_data['people'][0]['pose_keypoints_2d']\n",
    "            else: # fill in empty frames w/ 0s\n",
    "                pose_keypoints_2d = [0 for i in range(75)]\n",
    "\n",
    "            part_data = {}\n",
    "            for index in BODY25_MAPPING:\n",
    "                keypoint_index = index * POINTS_PER_PART\n",
    "                part_data[index] = pose_keypoints_2d[keypoint_index : keypoint_index + POINTS_PER_PART]\n",
    "                if len(part_data[index]) != 0: #normalize\n",
    "                    part_data[index][0] /= vm.width\n",
    "                    part_data[index][1] /= vm.height\n",
    "\n",
    "            frame_list.append(part_data)\n",
    "    return frame_list\n",
    "    \n",
    "data_list = []\n",
    "for path_to_json, vm in zip(json_dirs, video_metadata_intel):\n",
    "    keypoint_files = [os.path.join(path_to_json, pos_json) for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "    data_list.append(load_openpose_data(keypoint_files, vm))\n",
    "    \n",
    "#TODO expand for more than one vid\n",
    "frame_list = data_list[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate videos with open pose data, bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_op_bbox(frame):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        frame: dictionary of joint indices to normalized coords [x, y, conf]. ie {0: [.5, .5, .98]}\n",
    "    Returns 4 normalized bounding box coordinates x1, x2, y1, y2\n",
    "    \"\"\"\n",
    "    x1 = 1\n",
    "    x2 = 0\n",
    "    y1 = 1\n",
    "    y2 = 0\n",
    "    for key in frame:\n",
    "        joint = frame[key]\n",
    "        if len(joint) != 0:\n",
    "            if (joint[0] != 0):\n",
    "                x1 = min(x1, joint[0])\n",
    "                x2 = max(x2, joint[0])\n",
    "            if (joint[1] != 0):\n",
    "                y1 = min(y1, joint[1])\n",
    "                y2 = max(y2, joint[1])\n",
    "        \n",
    "    return x1, x2, y1, y2\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create intervalset mapping w/ pose visualizing data\n",
    "\n",
    "vm = video_metadata_intel[test_id]\n",
    "bboxes = [get_op_bbox(frame) for frame in frame_list]\n",
    "interval_mapping = IntervalSetMapping({\n",
    "        vm.id: IntervalSet([\n",
    "            Interval(\n",
    "                Bounds3D(\n",
    "                    t1 = frame_num / vm.fps,\n",
    "                    t2 = (frame_num + 1) / vm.fps,\n",
    "                    x1 = bboxes[frame_num][0],\n",
    "                    x2 = bboxes[frame_num][1],\n",
    "                    y1 = bboxes[frame_num][2],\n",
    "                    y2 = bboxes[frame_num][3]\n",
    "                ),\n",
    "                \n",
    "                {'spatial_type': SpatialType_Keypoints(),\n",
    "                    'metadata': {\n",
    "                        # This function can also parse faces and hands\n",
    "                        'pose': Metadata_Keypoints(pose, BODY25_EDGES)\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "            )\n",
    "            for frame_num, pose in enumerate(frame_list)\n",
    "        ])\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize OP data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c842fc6fe42428bad7a48b3c544d57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VGridWidget(vgrid_spec={'compressed': True, 'data': b'x\\x9c\\xec\\xbd\\xdb\\x8el\\xc9\\x8d\\xa6\\xf9*\\x85\\xbcN\\x08v>\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize\n",
    "def visualize_helper(video_metadata_intel, interval_mapping):\n",
    "    vgrid_spec = VGridSpec(\n",
    "        video_meta = video_metadata_intel,\n",
    "        vis_format = VideoBlockFormat(imaps = [\n",
    "            ('bboxes', interval_mapping)\n",
    "        ]),\n",
    "        video_endpoint = 'http://localhost:8000'\n",
    "    )\n",
    "    return VGridWidget(vgrid_spec = vgrid_spec.to_json_compressed())\n",
    "        \n",
    "visualize_helper(video_metadata_intel, interval_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate interval mapping for example dance move: hands up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_coord(interval, joint_idx, coord_idx):\n",
    "    md = interval['payload']['metadata']['pose'].to_json()\n",
    "    return md['args']['keypoints'][joint_idx][coord_idx]\n",
    "              \n",
    "Rwrist = 4\n",
    "Lwrist = 7\n",
    "Neck = 15 # this is actually eyes but ... forget it\n",
    "hands_up = interval_mapping.filter(lambda interval: \n",
    "                                   get_coord(interval, Rwrist, 1) < get_coord(interval, Neck, 1)\n",
    "                                  and  get_coord(interval, Lwrist, 1) < get_coord(interval, Neck, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720e35f8264d4b2287e058ca238b9c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VGridWidget(vgrid_spec={'compressed': True, 'data': b'x\\x9c\\xec\\xbd\\xdd\\xae4\\xd7\\x91\\x1c\\xfa*\\x03^\\x13\\xc2\\xfa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_helper(video_metadata_intel, hands_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand annotate video for this move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HAND ANNOTATE\n",
    "vgrid_spec = VGridSpec(\n",
    "        video_meta = video_metadata_intel,\n",
    "        vis_format = VideoBlockFormat(imaps = [\n",
    "            ('bboxes', interval_mapping)\n",
    "        ]),\n",
    "        video_endpoint = 'http://localhost:8000'\n",
    "    )\n",
    "widget =  VGridWidget(vgrid_spec = vgrid_spec.to_json_compressed()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c7672db6ee484b85274eca74435a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VGridWidget(vgrid_spec={'compressed': True, 'data': b'x\\x9c\\xec\\xbd\\xdb\\x8el\\xc9\\x8d\\xa6\\xf9*\\x85\\xbcN\\x08v>\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# widget_output = hands_up[0].get_intervals()[0]['payload']\n",
    "output_widget = True\n",
    "widget_file = '7th_sense_mayee_annotated.json'\n",
    "if output_widget:\n",
    "    with open(widget_file, 'w') as f:\n",
    "        json.dump(widget.label_state, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blocks_selected': {},\n",
       " 'block_labels': {'0': {'captions_selected': [],\n",
       "   'new_intervals': [{'bounds': {'t1': 17.502203,\n",
       "      't2': 17.725549,\n",
       "      'bbox': {'x1': 0, 'x2': 1, 'y1': 0, 'y2': 1}},\n",
       "     'data': {'spatial_type': {'args': {}}, 'metadata': {}}},\n",
       "    {'bounds': {'t1': 21.075739,\n",
       "      't2': 21.745777,\n",
       "      'bbox': {'x1': 0, 'x2': 1, 'y1': 0, 'y2': 1}},\n",
       "     'data': {'spatial_type': {'args': {}}, 'metadata': {}}},\n",
       "    {'bounds': {'t1': 24.22267,\n",
       "      't2': 24.22267001864799,\n",
       "      'bbox': {'x1': 0, 'x2': 1, 'y1': 0, 'y2': 1}},\n",
       "     'data': {'spatial_type': {'args': {}}, 'metadata': {}}},\n",
       "    {'bounds': {'t1': 51.055636,\n",
       "      't2': 52.160523,\n",
       "      'bbox': {'x1': 0, 'x2': 1, 'y1': 0, 'y2': 1}},\n",
       "     'data': {'spatial_type': {'args': {}}, 'metadata': {}}},\n",
       "    {'bounds': {'t1': 53.896773,\n",
       "      't2': 54.528137,\n",
       "      'bbox': {'x1': 0, 'x2': 1, 'y1': 0, 'y2': 1}},\n",
       "     'data': {'spatial_type': {'args': {}}, 'metadata': {}}},\n",
       "    {'bounds': {'t1': 65.261324,\n",
       "      't2': 66.050529,\n",
       "      'bbox': {'x1': 0, 'x2': 1, 'y1': 0, 'y2': 1}},\n",
       "     'data': {'spatial_type': {'args': {}}, 'metadata': {}}},\n",
       "    {'bounds': {'t1': 34.032665,\n",
       "      't2': 34.506188,\n",
       "      'bbox': {'x1': 0, 'x2': 1, 'y1': 0, 'y2': 1}},\n",
       "     'data': {'spatial_type': {'args': {}}, 'metadata': {}}},\n",
       "    {'bounds': {'t1': 45.239374,\n",
       "      't2': 45.712897,\n",
       "      'bbox': {'x1': 0, 'x2': 1, 'y1': 0, 'y2': 1}},\n",
       "     'data': {'spatial_type': {'args': {}}, 'metadata': {}}},\n",
       "    {'bounds': {'t1': 48.396194,\n",
       "      't2': 48.711876,\n",
       "      'bbox': {'x1': 0, 'x2': 1, 'y1': 0, 'y2': 1}},\n",
       "     'data': {'spatial_type': {'args': {}}, 'metadata': {}}},\n",
       "    {'bounds': {'t1': 59.287221,\n",
       "      't2': 59.602903,\n",
       "      'bbox': {'x1': 0, 'x2': 1, 'y1': 0, 'y2': 1}},\n",
       "     'data': {'spatial_type': {'args': {}}, 'metadata': {}}}]}}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TIME TO START EVALUATING YO\n",
    "# with open(widget_file, 'w') as f:\n",
    "#     widget_labels = json.loads(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with segment length = 0.1s\n",
      "Recall: 0.30357142857142855\n",
      "Precision: 0.34\n",
      "F1 score: 0.32075471698113206\n",
      "Accuracy: 0.8953488372093024\n",
      "\n",
      "Evaluating with segment length = 0.3s\n",
      "Recall: 0.391304347826087\n",
      "Precision: 0.3103448275862069\n",
      "F1 score: 0.34615384615384615\n",
      "Accuracy: 0.8521739130434782\n",
      "\n",
      "Evaluating with segment length = 0.5s\n",
      "Recall: 0.6666666666666666\n",
      "Precision: 0.4166666666666667\n",
      "F1 score: 0.5128205128205129\n",
      "Accuracy: 0.8623188405797102\n",
      "\n",
      "Evaluating with segment length = 0.7s\n",
      "Recall: 0.75\n",
      "Precision: 0.375\n",
      "F1 score: 0.5\n",
      "Accuracy: 0.8181818181818182\n",
      "\n",
      "Evaluating with segment length = 0.9s\n",
      "Recall: 0.5454545454545454\n",
      "Precision: 0.3333333333333333\n",
      "F1 score: 0.41379310344827586\n",
      "Accuracy: 0.7792207792207793\n",
      "\n",
      "Evaluating with segment length = 1.1s\n",
      "Recall: 0.7272727272727273\n",
      "Precision: 0.42105263157894735\n",
      "F1 score: 0.5333333333333333\n",
      "Accuracy: 0.7777777777777778\n",
      "\n",
      "Evaluating with segment length = 1.3s\n",
      "Recall: 0.8\n",
      "Precision: 0.5\n",
      "F1 score: 0.6153846153846154\n",
      "Accuracy: 0.8113207547169812\n",
      "\n",
      "Evaluating with segment length = 1.5s\n",
      "Recall: 0.9\n",
      "Precision: 0.5294117647058824\n",
      "F1 score: 0.6666666666666667\n",
      "Accuracy: 0.8043478260869565\n",
      "\n",
      "Evaluating with segment length = 1.7s\n",
      "Recall: 0.9\n",
      "Precision: 0.6\n",
      "F1 score: 0.7200000000000001\n",
      "Accuracy: 0.8292682926829268\n",
      "\n",
      "Evaluating with segment length = 1.9s\n",
      "Recall: 0.8\n",
      "Precision: 0.5714285714285714\n",
      "F1 score: 0.6666666666666666\n",
      "Accuracy: 0.7837837837837838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np \n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score #inputs: y_truth, y_predict\n",
    "\n",
    "def get_action_segments(interval_list, vm, segment_length = 1):\n",
    "    action_segments = np.zeros(math.ceil((vm.num_frames / vm.fps) / segment_length))\n",
    "    for interval in interval_list:\n",
    "        bounds = interval['bounds']\n",
    "        t1 = bounds['t1'] \n",
    "        t2 = bounds['t2'] \n",
    "        while (t1 <= t2):\n",
    "            segment_idx = math.floor(t1 / segment_length)\n",
    "            action_segments[segment_idx] = 1 #mark as 1 for an event!\n",
    "            t1 += segment_length\n",
    "    return action_segments\n",
    "\n",
    "def evaluate(intervals_predict, intervals_truth, vm, segment_length):\n",
    "    y_predict = get_action_segments(intervals_predict, vm, segment_length)\n",
    "    y_truth = get_action_segments(intervals_truth, vm, segment_length)\n",
    "    print(\"Evaluating with segment length = {}s\".format(segment_length))\n",
    "    print(\"Recall: {}\".format(recall_score(y_truth, y_predict)))\n",
    "    print(\"Precision: {}\".format(precision_score(y_truth, y_predict)))\n",
    "    print(\"F1 score: {}\".format(f1_score(y_truth, y_predict)))\n",
    "    print(\"Accuracy: {}\".format(accuracy_score(y_truth, y_predict)))\n",
    "    print()\n",
    "    \n",
    "interval_list = widget.label_state['block_labels']['0']['new_intervals']\n",
    "vm = video_metadata_intel[-1]\n",
    "segment_length = .1 # in terms of seconds / times instead of # frames, since time is a constant measure across vids and is more intuitive\n",
    "\n",
    "rekall_labels = hands_up[test_id].get_intervals()\n",
    "hand_labels = interval_list\n",
    "\n",
    "for i in range(1, 20, 2):\n",
    "    segment_length = i / 10.0\n",
    "    evaluate(rekall_labels, hand_labels, vm, segment_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, try it with Gaussian smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with segment length = 0.1s, sigma = 0.5s\n",
      "Relative score (cont. dot product): 16.952506113138625\n",
      "\n",
      "Evaluating with segment length = 0.3s, sigma = 0.5s\n",
      "Relative score (cont. dot product): 8.605778758347089\n",
      "\n",
      "Evaluating with segment length = 0.5s, sigma = 0.5s\n",
      "Relative score (cont. dot product): 8.506300740621437\n",
      "\n",
      "Evaluating with segment length = 0.7s, sigma = 0.5s\n",
      "Relative score (cont. dot product): 7.709284480112926\n",
      "\n",
      "Evaluating with segment length = 0.9s, sigma = 0.5s\n",
      "Relative score (cont. dot product): 5.594144875584039\n",
      "\n",
      "Evaluating with segment length = 1.1s, sigma = 0.5s\n",
      "Relative score (cont. dot product): 7.223811368694495\n",
      "\n",
      "Evaluating with segment length = 1.3s, sigma = 0.5s\n",
      "Relative score (cont. dot product): 6.386163555918522\n",
      "\n",
      "Evaluating with segment length = 1.5s, sigma = 0.5s\n",
      "Relative score (cont. dot product): 7.218645117420377\n",
      "\n",
      "Evaluating with segment length = 1.7s, sigma = 0.5s\n",
      "Relative score (cont. dot product): 7.074620546078005\n",
      "\n",
      "Evaluating with segment length = 1.9s, sigma = 0.5s\n",
      "Relative score (cont. dot product): 6.912324396887922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_gaussian(intervals_predict, intervals_truth, vm, segment_length=1, sigma=.5, visualize=True):\n",
    "    y_predict_raw = get_action_segments(intervals_predict, vm, segment_length)\n",
    "    y_truth_raw = get_action_segments(intervals_truth, vm, segment_length)\n",
    "    \n",
    "    #smooth\n",
    "    y_predict = gaussian_filter(y_predict_raw, sigma)\n",
    "    y_truth = gaussian_filter(y_truth_raw, sigma)\n",
    "    \n",
    "    if visualize:\n",
    "        plt.scatter(range(len(y_predict_raw)), y_predict_raw, label=\"original\")\n",
    "        plt.scatter(range(len(y_predict)), y_predict, label=\"smoothed\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Predictions\")\n",
    "        plt.show()\n",
    "    \n",
    "    #calculate recall\n",
    "    print(\"Evaluating with segment length = {}s, sigma = {}s\".format(segment_length, sigma))\n",
    "    relative_score = y_predict.dot(y_truth)\n",
    "    print (\"Relative score (cont. dot product): {}\".format(relative_score))\n",
    "        \n",
    "    print()\n",
    "#         print([round(num,3) for num in y_predict])\n",
    "    \n",
    "#     print(\"Evaluating with segment length = {}s and gaussian weighting\".format(segment_length))\n",
    "#     print(\"Recall: {}\".format(recall_score(y_truth, y_predict)))\n",
    "#     print(\"Precision: {}\".format(precision_score(y_truth, y_predict)))\n",
    "#     print(\"F1 score: {}\".format(f1_score(y_truth, y_predict)))\n",
    "#     print()\n",
    "                              \n",
    "for i in range(1, 20, 2):\n",
    "    segment_length = i / 10.0\n",
    "    evaluate_gaussian(rekall_labels, hand_labels, vm, segment_length, visualize=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment w/ preprocessing / smoothing prediction data to human capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
